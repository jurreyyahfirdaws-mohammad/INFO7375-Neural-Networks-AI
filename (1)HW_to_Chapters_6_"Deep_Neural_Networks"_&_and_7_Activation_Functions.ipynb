{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFkLPpe+4yUg+4LKRpIKgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jurreyyahfirdaws-mohammad/INFO7375-Neural-Networks-AI/blob/main/(1)HW_to_Chapters_6_%22Deep_Neural_Networks%22_%26_and_7_Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Develop linear, ReLU, sigmoid, tanh, and softmax activation functions as a class for neural networks implementation.\n"
      ],
      "metadata": {
        "id": "mBfd_cjXRHkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "7VFF8hwEOhba"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AF:\n",
        "    @staticmethod\n",
        "    def lin(x):\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def rlu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def sig(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def th(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def sftmax(x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    af = AF()\n",
        "\n",
        "    vals = np.array([-3,-2, -1, 0, 1, 2,3])\n",
        "\n",
        "    lin_out = af.lin(vals)\n",
        "    rlu_out = af.rlu(vals)\n",
        "    sig_out = af.sig(vals)\n",
        "    th_out = af.th(vals)\n",
        "    sft_out = af.sftmax(vals)\n",
        "\n",
        "    print(\"Input Values:\", vals)\n",
        "    print(\"Linear Output:\", lin_out)\n",
        "    print(\"ReLU Output:\", rlu_out)\n",
        "    print(\"Sigmoid Output:\", sig_out)\n",
        "    print(\"Tanh Output:\", th_out)\n",
        "    print(\"Softmax Output:\", sft_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7A6bCWIP5XJ",
        "outputId": "7b545240-7bc6-4488-fb38-c77c94afadbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Values: [-3 -2 -1  0  1  2  3]\n",
            "Linear Output: [-3 -2 -1  0  1  2  3]\n",
            "ReLU Output: [0 0 0 0 1 2 3]\n",
            "Sigmoid Output: [0.04742587 0.11920292 0.26894142 0.5        0.73105858 0.88079708\n",
            " 0.95257413]\n",
            "Tanh Output: [-0.99505475 -0.96402758 -0.76159416  0.          0.76159416  0.96402758\n",
            "  0.99505475]\n",
            "Softmax Output: [0.0015683  0.00426308 0.01158826 0.03150015 0.0856263  0.2327564\n",
            " 0.6326975 ]\n"
          ]
        }
      ]
    }
  ]
}